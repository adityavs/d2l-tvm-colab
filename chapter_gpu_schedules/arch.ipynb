{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Architecture\n",
    "\n",
    ":label:`ch_gpu_arch`\n",
    "\n",
    "\n",
    "\n",
    "High-end GPUs often provide a significantly better performance over high-end CPUs. Despite the terminologies and programming paradigms are different between GPUs and CPUs. Their architectures are similar to each other, but GPU has a wider SIMD width and more cores. In this chapter, we will brief review the GPU architecture based on the CPU architecture presented in :numref:`ch_cpu_arch`.\n",
    "\n",
    "(FIXME, changed from V100 to T4 in CI..., also changed cpu...)\n",
    "\n",
    "The system we are using has a [Tesla T4](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf) GPUs, therefore we will illustrate the architecture based on V100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Product Name                    : Tesla T4\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q -i 0 | grep \"Product Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Multiprocessor\n",
    "\n",
    "A streaming multiprocessor (SM) roughly equals to a CPU core. The SM used by V100 is illustrated in :numref:`fig_gpu_sm`.\n",
    "\n",
    "![A streaming multiprocessor in Tesla V100](../img/gpu_sm.svg)\n",
    "\n",
    ":label:`fig_gpu_sm`\n",
    "\n",
    "\n",
    "As can be seen that, a SM has four partitions. In each partition, there is 16 arithmetic units (AU) for float32, which is also called FP32 CUDA core. So a SM has 64 FP32 AUs in total, which is able to execute 64 float32 operators (e.g. FMA) in each time.  Besides the register files and the instruction loader/decoders, a SM has 8 tensor cores. Each tensor core is able to execute a $4\\times 4$ float16 matrix product in each time. So each one, we can call it FP16 AU, counts for $2\\times 4^3=128$ operators per clock.\n",
    "\n",
    "Another difference is that SM only has a L1 cache, which similar to CPU's L1 cache. However, we can use the storage as a shared memory for all threads running on the SM. We know that the cache is controlled by both hardware and operation system, while we can explicitly create and delete memory on the shared memory, which gives us more flexibility to optimize the performance.\n",
    "\n",
    "## V100 Architecture\n",
    "\n",
    "The Tesla V100 card has a GV100 processor, which contains 80 SMs with a 6MB L2 cache shared by all SMs. It also ships with 16GB high-speed memory that is connected to the processor. The overall architecture is illustrated in :numref:`fig_gpu_v100`.\n",
    "\n",
    "![The Tesla V100 Architecture](../img/gpu_v100.svg)\n",
    "\n",
    ":label:`fig_gpu_v100`\n",
    "\n",
    "\n",
    "We compare the specification difference between the CPU and GPU used by this book in :numref:`tab_cpu_gpu_compare`. TODO(mli)\n",
    "\n",
    ":Compare the CPU and GPU used by this book.\n",
    "\n",
    "|Hardware | Intel E5-2686 v4 | Tesla V100 |\n",
    "|------|------|------|\n",
    "| Clock rate (GHz) | **3** | 1.53 |\n",
    "| # cores | 16 | **80** |\n",
    "| # FP64 AUs per core | 4 | **32** |\n",
    "| # FP32 AUs per core | 8 | **64** |\n",
    "| # FP16 AUs per core | ? | **8** |\n",
    "| cache per core (KB) | **320** | 128 |\n",
    "| shared cache (MB)| **45** | 6 |\n",
    "| Memory (GB) | **240** | 16 |\n",
    "| Max memory bandwidth (GB/sec) | 72 | **900** |\n",
    "| FP64 TFLOPS | 0.38 | **7.8** |\n",
    "| FP32 TFLOPS | 0.77 | **15.7** |\n",
    "| FP16 TFLOPS | ? | **125.3** |\n",
    "\n",
    ":label:`tab_cpu_gpu_compare`\n",
    "\n",
    "\n",
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}